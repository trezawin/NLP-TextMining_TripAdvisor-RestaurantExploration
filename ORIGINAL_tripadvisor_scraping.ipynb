{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"GwI8inRW-9m0","executionInfo":{"status":"ok","timestamp":1742124221257,"user_tz":-60,"elapsed":597,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}}},"outputs":[],"source":["# %pip install pandas\n","# %pip install numpy\n","# %pip install tqdm\n","# %pip install bs4\n","import requests\n","import pandas as pd\n","import re\n","import time\n","import numpy as np\n","import platform\n","import tqdm\n","from bs4 import BeautifulSoup\n","import random"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Om2H-8of-9m2","executionInfo":{"status":"ok","timestamp":1742125747994,"user_tz":-60,"elapsed":51,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}}},"outputs":[],"source":["class SearchEngine:\n","    # Constructeur\n","    # def __init__(self):\n","    #     self.base_url = \"https://www.tripadvisor.fr\"\n","    #     self.session = None\n","    #     self.cookies_list = [{'TADCID': '95got4bq91GmregYABQCrj-Ib21-TgWwDB4AzTFpg4J0pITsougolk1a7SaZZ7sYmkJu7KPOcqvM0xwqN1w93LttdFLFrqsYJ7w', 'TASameSite': '1', '__vt': 'M-4ijWLl6k7ybNR4ABQCjdMFtf3dS_auw5cMBDN7STJSRCH1nXChQ-WY5JlSnMR1uxIGdRWOqVv5tzhczv2Wi5nvq3oTCp7u5GoQG5I0EKASCq7B0TNG2mQVbmucgXvHPZ_YZpAWQnpr_ru3N91wLIMyxQ', 'TASID': 'A49FD32AED6A062F36B3FEFE390763C5', 'TAUnique': '%1%enc%3AdkjfOfw%2Ftm%2FlxUOBpOSBodxVTnjfkCfULJk9agUV4g3zZWtZ%2FjDJk605yvvNffQrNox8JbUSTxk%3D', 'datadome': 'M8Sgk8DaVoG_E_zlyrRsQ9uvHoNM93ifQS9R8PKSjGOo6Aqy6Zw5CJtbfX8jys20Iv2oN7NAJFAg6Nxh9nZYyZ4SJfCN_nRu~5g07XIDokW_jYeHO5ur5ekBFu_TMUFj'},\n","    #                          {'TADCID': '95got4bq91GmregYABQCrj-Ib21-TgWwDB4AzTFpg4J0pITsougolk1a7SaZZ7sYmkJu7KPOcqvM0xwqN1w93LttdFLFrqsYJ7w', 'TASameSite': '1', '__vt': 'M-4ijWLl6k7ybNR4ABQCjdMFtf3dS_auw5cMBDN7STJSRCH1nXChQ-WY5JlSnMR1uxIGdRWOqVv5tzhczv2Wi5nvq3oTCp7u5GoQG5I0EKASCq7B0TNG2mQVbmucgXvHPZ_YZpAWQnpr_ru3N91wLIMyxQ', 'TASID': 'A49FD32AED6A062F36B3FEFE390763C5', 'TAUnique': '%1%enc%3AdkjfOfw%2Ftm%2FlxUOBpOSBodxVTnjfkCfULJk9agUV4g3zZWtZ%2FjDJk605yvvNffQrNox8JbUSTxk%3D', 'datadome': 'M8Sgk8DaVoG_E_zlyrRsQ9uvHoNM93ifQS9R8PKSjGOo6Aqy6Zw5CJtbfX8jys20Iv2oN7NAJFAg6Nxh9nZYyZ4SJfCN_nRu~5g07XIDokW_jYeHO5ur5ekBFu_TMUFj'}]\n","    #     self.soup = None\n","    #     self.cookies = None\n","    #     self.rank = 0\n","    #     self.url = None\n","\n","    # # M√©thode d'affichage\n","    # def __str__(self):\n","    #     print(f\"SearchEngine(base_url='{self.base_url}')\")\n","    #     print(f\"Session: {self.session}\")\n","    #     print(f\"cookies: {self.cookies}\")\n","    #     print(f\"Soup: {self.soup}\")\n","    #     return \"\"\n","\n","    # # M√©thode de r√©cup√©ration de la session\n","    # def get_session(self):\n","    #     # Cr√©ation d'une nouvelle session si elle n'existe pas\n","    #     if not self.session:\n","    #         print(\"Creating a new session...\")\n","    #         self.session = requests.Session()\n","    #         self.cookies = np.random.choice(self.cookies_list)\n","    #         for name, value in self.cookies.items():\n","    #             self.session.cookies.set(name, value, domain='.tripadvisor.fr')\n","    #     else:\n","    #         print(\"Session already exists\")\n","\n","    def __init__(self):\n","        self.base_url = \"https://www.tripadvisor.fr\"\n","        self.session = None\n","        self.cookies_list = [\n","            {\"TADCID\": \"sample1\", \"TASameSite\": \"1\", \"datadome\": \"dummy-cookie1\"},\n","            {\"TADCID\": \"sample2\", \"TASameSite\": \"1\", \"datadome\": \"dummy-cookie2\"},\n","        ]\n","        self.cookies = None\n","        self.rank = 0\n","        self.soup = None\n","\n","    def get_session(self):\n","        if not self.session:\n","            print(\"üîÑ Creating new session\")\n","            self.session = requests.Session()\n","            self.cookies = random.choice(self.cookies_list)\n","            for name, value in self.cookies.items():\n","                self.session.cookies.set(name, value, domain=\".tripadvisor.fr\")\n","        else:\n","            print(\"‚ö†Ô∏è Session already exists\")\n","\n","    # M√©thode de r√©cup√©ration de l'agent utilisateur en fonction du syst√®me d'exploitation\n","    # def get_os_user_agent(self):\n","    #     os = platform.system()\n","    #     if os == \"Windows\":\n","    #         return \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/109.0\"\n","    #     elif os == \"Linux\":\n","    #         return \"Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n","    #     else:\n","    #         return \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/\"\n","\n","    # M√©thode d'√©x√©cution de la requ√™te\n","    def run(self, url, reviews=None):\n","        self.get_session()\n","        self.url = url\n","        user_agents = [\n","            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/122 Safari/537.36\",\n","            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Safari/605.1.15\",\n","            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/121 Safari/537.36\",\n","            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0\"\n","        ]\n","        user_agent = random.choice(user_agents)\n","\n","        # R√©cup√©ration de l'agent utilisateur en fonction de la requ√™te\n","        if reviews:\n","            # user_agent = self.get_os_user_agent()\n","            user_agent = user_agent\n","        else:\n","            user_agent = \"Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrower/27.0 Chrome/125.0.0.0 Mobile Safari/537.36\"\n","\n","        print(\"user-agent: {}\", user_agent)\n","        headers = {\n","            \"User-Agent\": user_agent,\n","            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n","            \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8\",\n","            \"Connection\": \"keep-alive\",\n","            \"DNT\": \"1\",\n","            \"Upgrade-Insecure-Requests\": \"1\",\n","            \"Referer\": \"https://www.tripadvisor.fr/\",\n","        }\n","        # headers = {\n","\n","        #     'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Gecko/20100101 Firefox/109.0\", # AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\n","        #     # 'User-Agent': \"Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrower/27.0 Chrome/125.0.0.0 Mobile Safari/537.36\",\n","        #     # 'User-Agent':  user_agent,\n","        #     'Accept': 'text/html, application/xhtml+xml, application/xml;q=0.9, image/avif, image/webp, image/apng, image/*,*/*;q=0.8',\n","        #     'Accept-Language': 'fr-FR,fr;q=0.9',\n","        #     'Connection': 'keep-alive',\n","        #     'Upgrade-Insecure-Requests': '1',\n","        #     'TE': 'Trailers',\n","        #     \"DNT\": \"1\",\n","        #     \"Origin\": \"https://www.tripadvisor.fr/\",\n","        #     'Referer' : url ,\n","\n","        # }\n","\n","        # R√©cup√©ration de la session\n","        # self.get_session()\n","\n","        # Mise √† jour des en-t√™tes\n","        # self.session.headers.update(headers)\n","        # time.sleep(1)\n","        # print(time.ctime())\n","        self.session.headers.update(headers)\n","        time.sleep(random.uniform(2, 5))\n","\n","        # Ex√©cution de la requ√™te\n","        # response = self.session.get(url, headers=headers, timeout=(6, 36))\n","        # time.sleep(1)\n","        # response = self.session.get(url, headers=headers, timeout=(6, 30))\n","\n","        # # V√©rification du code de statut\n","        # if response.status_code == 200:\n","        #     self.soup = BeautifulSoup(response.content, 'html.parser')\n","        #     return None\n","        # elif response.status_code == 404:\n","        #     print(\"404 Not Found\")\n","        #     return None\n","        # elif response.status_code == 403:\n","        #     print(\"403 Forbidden\")\n","        #     print(response.text)\n","        #     return None\n","        # elif response.status_code == 503:\n","        #     print(\"503 Service Unavailable\")\n","        #     return None\n","        # elif response.status_code == 500:\n","        #     print(\"500 Internal Server Error\")\n","        #     return None\n","        # elif response.status_code == 504:\n","        #     print(\"504 Gateway Timeout\")\n","        #     return None\n","        # elif response.status_code == 429:\n","        #     time.sleep(60)\n","        #     print(\"429 Too Many Requests\")\n","        #     self.rank += 1\n","        #     if self.rank < 5:\n","        #         return self.run(url)\n","        #     else:\n","        #         self.rank = 0\n","        #         return None\n","        # else:\n","        #     print(\"Unknown error\")\n","        #     return None\n","\n","        try:\n","            response = self.session.get(url, headers=headers, timeout=(6, 30))\n","        except Exception as e:\n","            print(f\"‚ùå Request failed: {e}\")\n","            return None\n","\n","        if response.status_code == 200:\n","            self.soup = BeautifulSoup(response.content, \"html.parser\")\n","            return\n","        elif response.status_code == 403:\n","            print(f\"üö´ 403 Forbidden at {url}\")\n","            return None\n","        elif response.status_code == 429:\n","            print(\"‚è≥ 429 Too Many Requests ‚Äî sleeping\")\n","            time.sleep(random.uniform(60, 90))\n","            return self.run(url, reviews=reviews)\n","        else:\n","            print(f\"‚ùå HTTP {response.status_code} at {url}\")\n","            return None\n","\n","    def get_next_url(self):\n","        if not self.soup:\n","            return None\n","        pagination = self.soup.find_all(\"div\", class_=\"mkNRT j\")\n","        if not pagination:\n","            return None\n","        attrs = [{\"aria-label\": \"Page suivante\"}, {\"aria-label\": \"Next page\"}]\n","        for attr in attrs:\n","            link = self.soup.find(\"a\", attrs=attr)\n","            if link:\n","                return link.get(\"href\")\n","        return None\n","    # # M√©thode de r√©cup√©ration de l'URL de la page suivante\n","    # def get_next_url(self):\n","    #     # Recherche de la pagination\n","    #     pagination = self.soup.find_all(\"div\", class_=\"mkNRT j\")\n","    #     if not pagination:\n","    #         return None\n","\n","    #     # Attributs possibles pour le bouton de la page suivante\n","    #     attrs_possible = [\n","    #         {\"aria-label\" : \"Page suivante\"},\n","    #         {\"data-smoke-attr\" : \"pagination-next-arrow\" },\n","    #         {\"aria-label\" : \"Next page\"},\n","    #     ]\n","\n","    #     # Recherche du bouton de la page suivante\n","    #     for page in pagination:\n","    #         page_elements = page.find(\"a\", attrs=attrs_possible[0]) or page.find(\"button\", attrs=attrs_possible[1]) or page.find(\"a\", attrs=attrs_possible[2])\n","    #         if page_elements:\n","    #             return page_elements.get(\"href\")\n","    #         else:\n","    #             print(\"No next page found\")\n","    #             return None\n","\n","\n","class restaurant_info_extractor(SearchEngine):\n","    def __init__(self):\n","        super().__init__()\n","        self.restaurant_info = {}\n","        self.reviews = []\n","\n","    def scrape_reviews(self, url):\n","        self.reviews = [] ####\n","        self.session = None\n","        self.run(url, reviews=True)\n","\n","        if not self.soup:\n","            print(f\"‚ùå Failed to fetch {url}\")\n","            return {}, [] ####\n","\n","        seen_reviews = set() ####\n","\n","        self.extract_reviews(self.soup, seen_reviews)#####\n","        next_page_href = self.get_next_url()\n","        page_count = 1\n","        retry_count = 0\n","        max_retries = 3 ###\n","\n","        while next_page_href:\n","            page_count += 1\n","            print(f\"‚û°Ô∏è Moving to page {page_count}\")\n","\n","            if page_count % 2 == 0:\n","                self.session = None\n","                self.get_session()\n","\n","            time.sleep(random.uniform(2, 5))\n","            self.run(self.base_url + next_page_href)\n","            if not self.soup or \"403 Forbidden\" in str(self.soup):\n","                retry_count += 1\n","                if retry_count > 3:\n","                    print(\"‚ùå Too many retries. Stopping pagination.\")\n","                    break\n","                print(\"üîÅ Retrying session...\")\n","                self.session = None\n","                self.get_session()\n","                time.sleep(random.uniform(3, 6))\n","                continue\n","            else:\n","                retry_count = 0\n","                self.extract_reviews(self.soup)\n","                print(f\"‚úîÔ∏è Total reviews collected so far: {len(self.reviews)}\")\n","                next_page_href = self.get_next_url()\n","\n","        return self.restaurant_info, self.reviews\n","\n","    def extract_reviews(self, soup, seen_reviews=None):\n","        reviews = soup.find_all(\"div\", class_=\"_c\")\n","        for review in reviews:\n","            try:\n","              user_tag = review.find(\"a\", class_=\"BMQDV _F Gv wSSLS SwZTJ FGwzt ukgoS\")\n","              user = user_tag.text.strip() if user_tag else None\n","              user_profile = user_tag['href'].replace('/Profile/', '') if user_tag and 'href' in user_tag.attrs else None\n","              title = review.find(\"div\", class_=\"biGQs _P fiohW qWPrE ncFvv fOtGX\")\n","              title = title.text.strip() if title else None\n","              content = review.find(\"span\", class_=\"JguWG\")\n","              content = content.text.strip() if content else None\n","              date_review = review.find(\"div\", class_=\"biGQs _P pZUbB ncFvv osNWb\")\n","              date_review = date_review.text.replace(\"R√©dig√© le\", \"\").strip() if date_review else None\n","              # Type of visit\n","              type_visit = review.find(\"span\", class_=\"DlAxN\")\n","              type_visit = type_visit.text.strip() if type_visit else None\n","              # Number of user contributions\n","              contrib_tag = review.find(\"div\", class_=\"vYLts\")\n","              num_contributions_tag = contrib_tag.find(\"span\", class_=\"b\") if contrib_tag else None\n","              num_contributions = int(num_contributions_tag.text.strip()) if num_contributions_tag else None\n","\n","\n","              # Extract rating\n","              rating_tag = review.find(\"div\", class_=\"OSBmi J k\")\n","              rating_title = rating_tag.find(\"title\").text if rating_tag and rating_tag.find(\"title\") else \"\"\n","              match = re.search(r\"\\d,\\d\", rating_title)\n","              rating = float(match.group().replace(\",\", \".\")) if match else None\n","\n","              # Deduplication check\n","              dedup_key = f\"{user_profile}_{title}\"\n","              if seen_reviews is not None and dedup_key in seen_reviews:\n","                  continue\n","              if seen_reviews is not None:\n","                  seen_reviews.add(dedup_key)\n","\n","              # ‚ûï Append review with restaurant_name\n","              self.reviews.append({\n","                  \"restaurant_name\":self.url,\n","                  \"user\": user,\n","                  \"user_profile\": user_profile,\n","                  \"title\": title,\n","                  \"review\": content,\n","                  \"rating\": rating,\n","                  \"date_review\": date_review,\n","                  \"type_visit\": type_visit,\n","                  \"num_contributions\": num_contributions\n","                  })\n","            except AttributeError:\n","              continue\n"]},{"cell_type":"code","source":["url = \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d28100759-Reviews-Bikube_Lyon_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n","# url=\"https://www.tripadvisor.fr/Restaurant_Review-g187147-d23908565-Reviews-Under_The_Sea_Restaurant_Ephemera-Paris_Ile_de_France.html\"\n","# url = \"https://www.tripadvisor.com/Restaurant_Review-g187497-d11906544-Reviews-Bodega_Biarritz-Barcelona_Catalonia.html\"\n","search = restaurant_info_extractor()\n","info, reviews = search.scrape_reviews(url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"NlyLpBTIRD32","executionInfo":{"status":"error","timestamp":1742126018645,"user_tz":-60,"elapsed":4350,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}},"outputId":"c598c29c-13f5-4033-b37a-c95406d7efa0"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Creating new session\n","user-agent: {} Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/122 Safari/537.36\n"]},{"output_type":"error","ename":"TypeError","evalue":"restaurant_info_extractor.extract_reviews() takes 2 positional arguments but 3 were given","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-7e9bdb5baf62>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# url = \"https://www.tripadvisor.com/Restaurant_Review-g187497-d11906544-Reviews-Bodega_Biarritz-Barcelona_Catalonia.html\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestaurant_info_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-35-7d8088341d2a>\u001b[0m in \u001b[0;36mscrape_reviews\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mseen_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mnext_page_href\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mpage_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: restaurant_info_extractor.extract_reviews() takes 2 positional arguments but 3 were given"]}]},{"cell_type":"code","source":["# prompt: print the size of reviews\n","\n","print(len(reviews))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_r5jEzc6TeBY","executionInfo":{"status":"ok","timestamp":1741723948365,"user_tz":-60,"elapsed":12,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}},"outputId":"9f9b62e3-656b-461b-ed76-201b72d2299d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8\n"]}]},{"cell_type":"code","execution_count":35,"metadata":{"id":"WCcySonR-9m3","executionInfo":{"status":"ok","timestamp":1742126010745,"user_tz":-60,"elapsed":42,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}}},"outputs":[],"source":["class restaurant_info_extractor(SearchEngine):\n","    # Constructeur\n","    def __init__(self):\n","        super().__init__()\n","        self.restaurant_info = {}\n","        self.reviews = []\n","        self.rank_info = 0\n","        self.review_number = 0\n","\n","    # def get_restaurant_info(self, soup):\n","    #     # Notes et avis\n","    #     notes_avis_section = soup.find('div', class_='QSyom f e Q3 _Z')\n","\n","    #     # Si la section des notes et avis n'est pas trouv√©e\n","    #     if not notes_avis_section:\n","    #         print(\"No notes found\")\n","    #         if self.rank_info < 2:\n","    #             print(\"No data found\")\n","    #             self.rank_info += 1\n","    #             for i in tqdm.tqdm(range(5)):\n","    #                 time.sleep(1)\n","    #             print(self.session.cookies)\n","    #             self.session.cookies.clear()\n","    #             self.session = None\n","    #             return self.scrape_info(self.url)\n","\n","    #         if self.rank_info == 2:\n","    #             self.rank_info = 0\n","    #             with open('restaurant_info.html', 'w', encoding='utf-8') as f:\n","    #                 f.write(str(soup))\n","    #             print(\"No data found correctly\")\n","\n","    #     notes = {}\n","    #     global_note = cuisine_note = service_note = quality_price_note = ambiance_note = None\n","\n","    #     # Si la section des notes et avis est trouv√©e\n","    #     if notes_avis_section:\n","    #         global_note_tag = notes_avis_section.find('span', class_='biGQs _P fiohW uuBRH')\n","    #         global_note = global_note_tag.text if global_note_tag else None\n","    #         print(global_note)\n","    #         other_notes_tags = notes_avis_section.find('div', class_='khxWm f e Q3')\n","    #         if other_notes_tags:\n","    #             other_notes_div = other_notes_tags.find_all('div', class_='YwaWb u f')\n","    #             if other_notes_div and len(other_notes_div) == 4:\n","    #                 cuisine_note = self.extract_note(other_notes_div[0].text)\n","    #                 service_note = self.extract_note(other_notes_div[1].text)\n","    #                 quality_price_note = self.extract_note(other_notes_div[2].text)\n","    #                 ambiance_note = self.extract_note(other_notes_div[3].text)\n","\n","    #     # D√©finition des notes\n","    #     notes = {\n","    #         'CUISINE': cuisine_note,\n","    #         'SERVICE': service_note,\n","    #         'RAPPORT QUALIT√â-PRIX': quality_price_note,\n","    #         'AMBIANCE': ambiance_note\n","    #     }\n","\n","    #     # R√©cup√©ration de la section des d√©tails\n","    #     details_section = soup.find('div', class_='MTwbb f e')\n","    #     if details_section:\n","    #         details = {}\n","    #         detail_items = details_section.find_all('div', class_='biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD')\n","\n","    #         labels = details_section.find_all('div', class_='Wf')\n","    #         for label, value in zip(labels, detail_items):\n","    #             key = label.find('div', class_='biGQs _P ncFvv NaqPn').text.strip()\n","    #             val = value.text.strip()\n","    #             details[key.upper()] = val\n","    #     else:\n","    #         details = {'Fourchette de prix': None , 'Cuisines': None, 'R√©gimes sp√©ciaux': None}\n","\n","    #     # R√©cup√©ration des coordonn√©es\n","    #     location_section = soup.find('div', class_='Zb w')\n","    #     if location_section:\n","    #         address_tag = location_section.find('a', href=True)\n","    #         address = address_tag.text.strip() if address_tag else None\n","    #         google_map = address_tag['href'] if address_tag else None\n","\n","    #         # Fonction pour extraire la latitude et la longitude de l'URL Google Maps\n","    #         def extract_lat_lon(href):\n","    #             match = re.search(r'@(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)', href)\n","    #             return match.groups() if match else (None, None)\n","    #         lat, lon = extract_lat_lon(google_map)\n","\n","    #         email_tag = soup.find('a', href=lambda href: href and href.startswith('mailto:'))\n","    #         email = email_tag['href'].split(':')[1] if email_tag else None\n","\n","    #         phone_tag = location_section.find('a', href=lambda x: x and x.startswith('tel:'))\n","    #         phone_numer = phone_tag['href'].split(':')[1] if phone_tag else None\n","    #     else:\n","    #         address = email = phone_numer = None\n","\n","    #     # Oragnisation des informations du restaurant\n","    #     restaurant_info = {\n","    #         'Notes et avis': {\n","    #             'NOTE GLOBALE': global_note,\n","    #             **notes\n","    #         },\n","    #         'D√©tails': details,\n","    #         'Emplacement et coordonn√©es': {\n","    #             'ADRESSE': address,\n","    #             'GOOGLE MAP': google_map,\n","    #             'LATITUDE': lat,\n","    #             'LONGITUDE': lon,\n","    #             'EMAIL': email,\n","    #             'TELEPHONE': phone_numer\n","    #         }\n","    #     }\n","    #     stars = self.michelin_star_finder(soup)\n","    #     restaurant_info['D√©tails']['√âTOILES MICHELIN'] = stars\n","    #     image_url = self.get_images(soup)\n","    #     restaurant_info['D√©tails']['IMAGE'] = image_url\n","    #     time.sleep(1)\n","    #     fonctionnalite , horaires , rank = self.google_scrapping_info(self.url)\n","    #     if fonctionnalite is None:\n","    #         if self.rank_info < 3:\n","    #             for i in tqdm.tqdm(range(3)):\n","    #                 time.sleep(1)\n","    #             self.google_scrapping_info(self.url)\n","    #             self.rank_info += 1\n","    #         else:\n","    #             self.rank_info = 0\n","    #     restaurant_info['D√©tails']['FONCTIONNALITE'] = fonctionnalite if fonctionnalite else None\n","    #     restaurant_info['D√©tails']['HORAIRES'] = horaires if horaires else None\n","    #     restaurant_info['D√©tails']['RANK'] = rank if rank else None\n","    #     if restaurant_info['D√©tails']['RANK'] is None:\n","    #         restaurant_info['D√©tails']['RANK'] = self.get_ranking(soup)\n","\n","    #     return restaurant_info\n","\n","    # def scrape_info(self, url):\n","    #     self.run(url)\n","    #     if not self.soup:\n","    #         print(f\"Failed to get restaurant page {url}\")\n","    #         return None\n","    #     self.restaurant_info = self.get_restaurant_info(self.soup)\n","    #     return self.restaurant_info\n","\n","    def scrape_reviews(self, url):\n","        self.reviews = [] ####\n","        self.session = None\n","        self.run(url, reviews=True)\n","\n","        if not self.soup:\n","            print(f\"‚ùå Failed to fetch {url}\")\n","            return {}, [] ####\n","\n","        seen_reviews = set() ####\n","\n","        self.extract_reviews(self.soup, seen_reviews)#####\n","        next_page_href = self.get_next_url()\n","        page_count = 1\n","        retry_count = 0\n","        max_retries = 3 ###\n","\n","        while next_page_href:\n","            page_count += 1\n","            print(f\"‚û°Ô∏è Moving to page {page_count}\")\n","\n","            if page_count % 2 == 0:\n","                self.session = None\n","                self.get_session()\n","\n","            time.sleep(random.uniform(2, 5))\n","            self.run(self.base_url + next_page_href)\n","            if not self.soup or \"403 Forbidden\" in str(self.soup):\n","                retry_count += 1\n","                if retry_count > 3:\n","                    print(\"‚ùå Too many retries. Stopping pagination.\")\n","                    break\n","                print(\"üîÅ Retrying session...\")\n","                self.session = None\n","                self.get_session()\n","                time.sleep(random.uniform(3, 6))\n","                continue\n","            else:\n","                retry_count = 0\n","                self.extract_reviews(self.soup)\n","                print(f\"‚úîÔ∏è Total reviews collected so far: {len(self.reviews)}\")\n","                next_page_href = self.get_next_url()\n","\n","        return self.restaurant_info, self.reviews\n","\n","    # def scrape_reviews(self, url):\n","    #     self.session = None\n","    #     self.run(url, reviews=True)\n","    #     if not self.soup:\n","    #         print(f\"Failed to get restaurant page {url}\")\n","    #         return None\n","    #     self.extract_reviews(self.soup)\n","    #     next_page_href = self.get_next_url()\n","    #     print(\"Next page : \" , next_page_href)\n","    #     while next_page_href:\n","    #         for i in tqdm.tqdm(range(5)):\n","    #                     time.sleep(1)\n","\n","    #         self.extract_reviews(self.soup)\n","    #         time.sleep(random.uniform(2, 5))\n","    #         run_ok = self.run(self.base_url + next_page_href)\n","    #         if run_ok is None:  # Check if run_ok is None\n","    #             break\n","    #         next_page_href = self.get_next_url()\n","\n","    #     return self.restaurant_info, self.reviews\n","\n","    # def extract_note(self,text):\n","    #                     match = re.search(r'\\d,\\d', text)\n","    #                     return match.group() if match else None\n","\n","    def extract_reviews(self, soup):\n","        reviews = soup.find_all(\"div\", class_=\"_c\")\n","        # It√©ration sur les avis\n","        for review in reviews:\n","            try:\n","                user = review.find(\"a\", class_=\"BMQDV _F Gv wSSLS SwZTJ FGwzt ukgoS\")\n","                user_profil = user['href'].replace('/Profile/', '') if user and 'href' in user.attrs else None\n","                title = review.find(\"div\", class_=\"biGQs _P fiohW qWPrE ncFvv fOtGX\").text.strip()\n","                avis = review.find(\"span\", class_=\"JguWG\")\n","                date_review = review.find(\"div\", class_=\"biGQs _P pZUbB ncFvv osNWb\").text.replace('R√©dig√© le ', '').strip()\n","                type_visits = review.find(\"span\", class_=\"DlAxN\")\n","                if type_visits is not None:\n","                    type_visits = type_visits.text\n","                else:\n","                    type_visits = \"No information\"\n","\n","                # Extraire la note de l'avis\n","                rating_tag = review.find(\"div\", class_=\"OSBmi J k\")\n","                rating_title = rating_tag.find(\"title\").text.strip() if rating_tag and rating_tag.find(\"title\") else None\n","                rating = float(re.search(r'\\d,\\d', rating_title).group().replace(',', '.')) if re.search(r'\\d,\\d', rating_title) else None\n","\n","                contrib_container = review.find(\"div\", class_=\"vYLts\")\n","                num_contributions_tag = contrib_container.find(\"span\", class_=\"b\") if contrib_container else None\n","                num_contributions = int(num_contributions_tag.text.strip()) if num_contributions_tag else 0\n","                print(\"user\", user.text)\n","                print(\"user_profil\", user_profil)\n","                print(\"date_review\", date_review)\n","                print(\"avis\",   avis.text.strip())\n","\n","                self.reviews.append({\n","                    'user': user.text.strip() if user else None,\n","                    'user_profile': user_profil if user_profil else None,\n","                    'date_review': date_review if date_review else None,\n","                    'title': title if title else None,\n","                    'rating': rating,\n","                    'type_visit': type_visits,\n","                    'num_contributions': num_contributions,\n","                    'review': avis.text if avis else None\n","\n","                })\n","\n","                self.review_number += 1\n","\n","            except AttributeError:\n","                continue\n","    def michelin_star_finder(self,soup):\n","        if soup.find('img', src=\"https://static.tacdn.com/img2/restaurant-awards/michelin/1-Star.svg\"):\n","            return 1\n","        elif soup.find('img', src=\"https://static.tacdn.com/img2/restaurant-awards/michelin/2-Stars.svg\"):\n","            return 2\n","        elif soup.find('img', src=\"https://static.tacdn.com/img2/restaurant-awards/michelin/3-Stars.svg\"):\n","            return 3\n","        else:\n","            return 0\n","\n","    def get_images(self, soup):\n","            photo_viewer_div = soup.find('div', {'data-section-signature': 'photo_viewer'})\n","            if photo_viewer_div:\n","                media_tags = photo_viewer_div.find( 'img')\n","\n","                # Extraction des URLs des attributs srcset\n","                srcset_urls = []\n","                media_tags\n","                srcset = media_tags.get('srcset')\n","                src = media_tags.get('src')\n","                if srcset:\n","                    urls = [s.split(' ')[0] for s in srcset.split(',')]\n","                    srcset_urls.extend(urls)\n","                    first_url = srcset_urls[0]\n","                    cleaned_url = first_url.split('?')[0]\n","\n","                elif src:\n","                    urls = [s.split(' ')[0] for s in src.split(',')]\n","                    srcset_urls.extend(urls)\n","                    first_url = srcset_urls[0]\n","                    cleaned_url  = first_url.split('?')[0]\n","                else :\n","                    cleaned_url = None\n","            else:\n","                cleaned_url = None\n","\n","            return cleaned_url\n","\n","    def google_scrapping_info(self, url):\n","        soup_google = self.get_soup(url)\n","        fonctionnalite = self.get_fonctionnalite(soup_google)\n","        horaires = self.get_workdays(soup_google)\n","        rank = self.get_ranking(soup_google)\n","        return  fonctionnalite, horaires, rank\n","\n","    def get_soup(self,url):\n","\n","        # D√©finition des en-t√™tes\n","        header = {\n","            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36\",  #\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n","                    'Accept': 'text/html, application/xhtml+xml, application/xml;q=0.9, image/avif, image/webp, image/apng, image/*,*/*;q=0.8',\n","                    'Accept-Language': 'fr-FR,fr;q=0.9',\n","                    'Connection': 'keep-alive',\n","                    'Upgrade-Insecure-Requests': '1',\n","                    'TE': 'Trailers',\n","                    \"DNT\": \"1\",\n","                    \"Origin\": \"https://www.tripadvisor.fr/\",\n","                    'Referer' : url ,\n","\n","        }\n","\n","        # Ex√©cution de la requ√™te\n","        response = requests.get(url, headers=header, timeout=(3.05, 27), params={\"_\": str(int(time.time()))})\n","        if response.status_code != 200:\n","            print(f\"Failed to get page {url}\")\n","            return None\n","        return BeautifulSoup(response.text, 'html.parser')\n","\n","    def get_fonctionnalite(self, soup_google):\n","\n","        # Recherche de la section des fonctionnalit√©s\n","        fonctionnalite = soup_google.find('div', class_='kYFok f e Q1 BUDdf')\n","        if not fonctionnalite:\n","            return None\n","        features = []\n","        feature_containers = fonctionnalite.find_all('div', class_='fQXjj')\n","        if not feature_containers:\n","            return None\n","\n","        # Recherche des fonctionnalit√©s\n","        for container in feature_containers:\n","            feature_text = container.find('span')\n","            if feature_text and feature_text.text.strip():\n","                features.append(feature_text.text.strip())\n","        fonct_text = \"\"\n","        for feature in features:\n","            fonct_text += f\"{feature}; \"\n","        return fonct_text\n","\n","    def get_workdays(self,soup_google):\n","        hours_div = soup_google.find('div', class_='OlkEn AdWFC')\n","        if not hours_div:\n","            return None\n","        hours_div_plus = hours_div.select('div[class=\"f\"]')\n","        if not hours_div_plus:\n","            return None\n","        horaires = {}\n","\n","        # Parcours des blocs pour extraire les informations\n","        for div in hours_div_plus:\n","            jour_div = div.find('div', class_='klgPI Nk')\n","            jours_precis = jour_div.find('div', class_='fOtGX')\n","            if jours_precis:\n","                jour = jours_precis.text.strip()\n","            else:\n","                continue\n","\n","            # Extraction des horaires o√π l'indication est Ferm√©\n","            horaires_div = div.find('div', class_='f e')\n","\n","            horaires[jour] = []\n","            if horaires_div:\n","                horaires_span = horaires_div.find_all('span')\n","                for span in horaires_span:\n","                    horaires[jour].append(span.text)\n","            else:\n","                horaires[jour].append(\"Ferm√©\")\n","\n","        # Mise en forme des horaires\n","        horaires_text = \"\"\n","        for jour, horaire in horaires.items():\n","            horaires_text += f\"{jour}: {', '.join(horaire)}; \"\n","        return horaires_text\n","\n","    def get_ranking(self,soup_google):\n","        ranking = soup_google.find(attrs={\"data-test-target\": \"restaurant-detail-info\"})\n","        if not ranking:\n","            return None\n","        ranking = ranking.find('span', class_='ffHqI')\n","        if not ranking:\n","            return None\n","        ranking = ranking.text\n","        ranking = ranking.replace('\\u202f', '')\n","        numbers  = re.findall(r'\\d+', ranking)\n","\n","        if len(numbers) >= 2:\n","            rank = int(numbers[0])\n","        else:\n","            rank = None\n","\n","        return rank"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXIDjPCf-9m4","executionInfo":{"status":"ok","timestamp":1742125236999,"user_tz":-60,"elapsed":2216,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}},"outputId":"26c21f8b-c441-46a5-ad20-7e244fe721a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["user-agent: {} Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/121 Safari/537.36\n","Creating a new session...\n","403 Forbidden\n","<html lang=\"fr\"><head><title>tripadvisor.fr</title><style>#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}</style></head><body style=\"margin:0\"><p id=\"cmsg\">Please enable JS and disable any ad blocker</p><script data-cfasync=\"false\">var dd={'rt':'c','cid':'AHrlqAAAAAMA66UlEwOKEVsATYV5uw==','hsh':'2F05D671381DB06BEE4CC52C7A6FD3','t':'fe','qp':'','s':46694,'e':'eee74528283293d4b77afa0ab6e79b2ceec76622a8e905eca9b93655f659f159','host':'geo.captcha-delivery.com','cookie':'fHegK1ImDNoOAgAfEpd5Yq4AjjYIHVJ3Ss18oU2WGZ3PyjgUomz0C8whN36dyhnsnIpcai570FupQ6CYXpRfquFGsB9IN7BNnbjYEsa3m7WDuwxi5IB22Zod8tm6QUus'}</script><script data-cfasync=\"false\" src=\"https://ct.captcha-delivery.com/c.js\"></script></body></html>\n","‚ùå Failed to fetch https://www.tripadvisor.fr/Restaurant_Review-g187265-d28100759-Reviews-Bikube_Lyon_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"]}],"source":["url = \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d28100759-Reviews-Bikube_Lyon_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n","# url=\"https://www.tripadvisor.fr/Restaurant_Review-g187147-d23908565-Reviews-Under_The_Sea_Restaurant_Ephemera-Paris_Ile_de_France.html\"\n","search = restaurant_info_extractor()\n","info, reviews = search.scrape_reviews(url)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjM9vHn--9m4"},"outputs":[],"source":["info = search.scrape_info(url)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTWG05Zw-9m4"},"outputs":[],"source":["info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nl5_Z2Ic-9m4"},"outputs":[],"source":["reviews"]},{"cell_type":"code","source":["pip install \"httpx[http2,brotli]\" parsel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PG65ps1TLdDk","executionInfo":{"status":"ok","timestamp":1741645802920,"user_tz":-60,"elapsed":5153,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}},"outputId":"9b2dd503-7ce8-4ef0-9e11-d4057ccb188f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting parsel\n","  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: httpx[brotli,http2] in /usr/local/lib/python3.11/dist-packages (0.28.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2]) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2]) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2]) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2]) (3.10)\n","Collecting brotli (from httpx[brotli,http2])\n","  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n","Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2]) (4.2.0)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx[brotli,http2]) (0.14.0)\n","Collecting cssselect>=1.2.0 (from parsel)\n","  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n","Collecting jmespath (from parsel)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from parsel) (5.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from parsel) (24.2)\n","Collecting w3lib>=1.19.0 (from parsel)\n","  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2]) (6.1.0)\n","Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2]) (4.1.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx[brotli,http2]) (1.3.1)\n","Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n","Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n","Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n","Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Installing collected packages: brotli, w3lib, jmespath, cssselect, parsel\n","Successfully installed brotli-1.1.0 cssselect-1.3.0 jmespath-1.0.1 parsel-1.10.0 w3lib-2.3.1\n"]}]},{"cell_type":"code","source":["import asyncio\n","import json\n","import math\n","from typing import List, Dict, Optional\n","from httpx import AsyncClient, Response\n","from parsel import Selector\n","\n","client = AsyncClient(\n","    headers={\n","        # use same headers as a popular web browser (Chrome on Windows in this case)\n","        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n","        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n","        \"Accept-Language\": \"en-US,en;q=0.9\",\n","    },\n","    follow_redirects=True\n",")\n","\n","def parse_hotel_page(result: Response) -> Dict:\n","    \"\"\"parse hotel data from hotel pages\"\"\"\n","    selector = Selector(result.text)\n","    # Find the script tag containing 'aggregateRating'\n","    script_tag = selector.xpath(\"//script[contains(text(),'aggregateRating')]\").get()\n","\n","    # Check if script_tag is found before proceeding\n","    if script_tag:\n","        # Extract the JSON data from the script tag\n","        basic_data_str = re.search(r'({.*?\"aggregateRating\".*?})', script_tag).group(1)\n","        basic_data = json.loads(basic_data_str)\n","    else:\n","        # Handle the case where the script tag is not found\n","        basic_data = {}  # Or raise an exception, depending on your needs\n","\n","    description = selector.css(\"div.fIrGe._T::text\").get()\n","    amenities = []\n","    for feature in selector.xpath(\"//div[contains(@data-test-target, 'amenity')]/text()\"):\n","        amenities.append(feature.get())\n","    reviews = []\n","    for review in selector.xpath(\"//div[@data-reviewid]\"):\n","        title = review.xpath(\".//div[@data-test-target='review-title']/a/span/span/text()\").get()\n","        text = \"\".join(review.xpath(\".//span[contains(@data-automation, 'reviewText')]/span/text()\").extract())\n","        rate = review.xpath(\".//div[@data-test-target='review-rating']/span/@class\").get()\n","        rate = (int(rate.split(\"ui_bubble_rating\")[-1].split(\"_\")[-1].replace(\"0\", \"\"))) if rate else None\n","        trip_data = review.xpath(\".//span[span[contains(text(),'Date of stay')]]/text()\").get()\n","        reviews.append({\n","            \"title\": title,\n","            \"text\": text,\n","            \"rate\": rate,\n","            \"tripDate\": trip_data\n","        })\n","\n","    return {\n","        \"basic_data\": basic_data,\n","        \"description\": description,\n","        \"featues\": amenities,\n","        \"reviews\": reviews\n","    }\n","\n","\n","async def scrape_hotel(url: str, max_review_pages: Optional[int] = None) -> Dict:\n","    \"\"\"Scrape hotel data and reviews\"\"\"\n","    first_page = await client.get(url)\n","    assert first_page.status_code == 403, \"request is blocked\" #Change this assert statement for better performance\n","    print(first_page)\n","    hotel_data = parse_hotel_page(first_page)\n","    print(hotel_data)\n","\n","    # get the number of total review pages\n","    _review_page_size = 10\n","    total_reviews = int(hotel_data[\"basic_data\"].get(\"aggregateRating\", {}).get(\"reviewCount\", 0)) #Handle potential KeyError\n","    total_review_pages = math.ceil(total_reviews / _review_page_size)\n","\n","    # get the number of review pages to scrape\n","    if max_review_pages and max_review_pages < total_review_pages:\n","        total_review_pages = max_review_pages\n","\n","    # scrape all review pages concurrently\n","    review_urls = [\n","        # note: \"or\" stands for \"offset reviews\"\n","        url.replace(\"-Reviews-\", f\"-Reviews-or{_review_page_size * i}-\")\n","        for i in range(1, total_review_pages)\n","    ]\n","    for response in asyncio.as_completed(review_urls):\n","        data = parse_hotel_page(await response)\n","        hotel_data[\"reviews\"].extend(data[\"reviews\"])\n","    print(f\"scraped one hotel data with {len(hotel_data['reviews'])} reviews\")\n","    return hotel_data"],"metadata":{"id":"_uta3QSbLPYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["async def run():\n","    hotel_data = await scrape_hotel(\n","        url=\"https://www.tripadvisor.com/Hotel_Review-g190327-d264936-Reviews-1926_Hotel_Spa-Sliema_Island_of_Malta.html\"\n","    )\n","    # print the result in JSON format\n","    print(json.dumps(hotel_data, indent=2))\n","\n","if __name__ == \"__main__\":\n","    # asyncio.run(run()) # This line is causing the error\n","    await run() # Fix: Replace with await run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xxQB-plSLmHd","executionInfo":{"status":"ok","timestamp":1741646438355,"user_tz":-60,"elapsed":82,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}},"outputId":"1cbb13da-56a5-4895-9a48-ed6d80fbd41d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<Response [403 Forbidden]>\n","{'basic_data': {}, 'description': None, 'featues': [], 'reviews': []}\n","scraped one hotel data with 0 reviews\n","{\n","  \"basic_data\": {},\n","  \"description\": null,\n","  \"featues\": [],\n","  \"reviews\": []\n","}\n"]}]},{"cell_type":"code","source":["!pip install selenium"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJCNpekmUJyA","executionInfo":{"status":"ok","timestamp":1741648083464,"user_tz":-60,"elapsed":5454,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}},"outputId":"6f03edcd-511d-4b1c-b57a-12e4c8e2f9e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting selenium\n","  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n","Collecting trio~=0.17 (from selenium)\n","  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting trio-websocket~=0.9 (from selenium)\n","  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n","Collecting outcome (from trio~=0.17->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n","Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n","Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n","Successfully installed outcome-1.3.0.post0 selenium-4.29.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import random\n","\n","# TripAdvisor restaurant review URL (Update this with the actual restaurant URL)\n","url = \"https://www.tripadvisor.com/Restaurant_Review-g187497-d739472-Reviews-Cerveceria_Catalana-Barcelona_Catalonia.html\"\n","\n","# User-Agent rotation to mimic real browsers\n","user_agents = [\n","    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n","    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n","    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n","]\n","\n","# Free proxy list (replace with fresh working proxies)\n","proxies = [\n","    \"http://45.77.195.146:8080\",\n","    \"http://190.61.88.147:8080\",\n","    \"http://103.231.78.36:80\",\n","]\n","\n","# Choose a random user-agent and proxy\n","headers = {\"User-Agent\": random.choice(user_agents)}\n","proxy = {\"http\": random.choice(proxies), \"https\": random.choice(proxies)}\n","\n","# Send request\n","response = requests.get(url, headers=headers, proxies=proxy)\n","\n","# Check response\n","if response.status_code == 200:\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","    reviews = soup.find_all(\"q\", class_=\"XllAv H4 _a\")  # Check correct class using soup.prettify()\n","\n","    for i, review in enumerate(reviews[:5], 1):\n","        print(f\"Review {i}: {review.text.strip()}\")\n","else:\n","    print(f\"Failed to fetch page, Status Code: {response.status_code}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"BURVqiX_Ttmk","executionInfo":{"status":"error","timestamp":1741648915457,"user_tz":-60,"elapsed":85578,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}},"outputId":"c3ae29fc-572f-457f-bb2c-6a4716618abe"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-89-5471f86acfef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Send request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Check response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhttp_tunnel_required\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     self._raise_timeout(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy_headers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         )\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseHTTPSConnection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0mtls_in_tls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \"\"\"\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import random\n","\n","# Function to get a list of fresh proxies\n","def get_proxies():\n","    url = \"https://free-proxy-list.net/\"  # Free proxy source\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","    proxies = []\n","    for row in soup.select(\"table tbody tr\"):\n","        columns = row.find_all(\"td\")\n","        ip = columns[0].text.strip()\n","        port = columns[1].text.strip()\n","        https = columns[6].text.strip()\n","\n","        if https == \"yes\":  # Use only HTTPS proxies\n","            proxy = f\"http://{ip}:{port}\"\n","            proxies.append(proxy)\n","\n","    return proxies\n","\n","# Function to test if a proxy is working\n","def test_proxy(proxy):\n","    test_url = \"https://httpbin.org/ip\"  # Checks IP visibility\n","    try:\n","        response = requests.get(test_url, proxies={\"http\": proxy, \"https\": proxy}, timeout=5)\n","        if response.status_code == 200:\n","            return True\n","    except requests.exceptions.RequestException:\n","        return False\n","    return False\n","\n","# Get fresh proxies\n","proxies = get_proxies()\n","print(f\"Found {len(proxies)} proxies. Testing...\")\n","\n","# Test and filter working proxies\n","working_proxies = [proxy for proxy in proxies if test_proxy(proxy)]\n","\n","# Show results\n","if working_proxies:\n","    print(f\"‚úÖ Working Proxies ({len(working_proxies)} found):\")\n","    for p in working_proxies:\n","        print(p)\n","else:\n","    print(\"‚ùå No working proxies found. Try again later.\")\n","\n","# Example: Using a random working proxy\n","if working_proxies:\n","    selected_proxy = random.choice(working_proxies)\n","    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","    url = \"https://www.tripadvisor.com/\"  # Change to your target site\n","\n","    print(f\"\\nUsing proxy: {selected_proxy}\")\n","    response = requests.get(url, headers=headers, proxies={\"http\": selected_proxy, \"https\": selected_proxy})\n","\n","    if response.status_code == 200:\n","        print(\"‚úÖ Successfully accessed TripAdvisor!\")\n","    else:\n","        print(f\"‚ùå Failed, Status Code: {response.status_code}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"akz3_j9EVaRO","executionInfo":{"status":"error","timestamp":1741648961510,"user_tz":-60,"elapsed":428,"user":{"displayName":"Treza Bawm Win","userId":"06512130008284013936"}},"outputId":"b3b4fdb8-8c61-4cc0-e0f1-c917b26df70e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-6d94473c2fb1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Get fresh proxies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_proxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(proxies)} proxies. Testing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-91-6d94473c2fb1>\u001b[0m in \u001b[0;36mget_proxies\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table tbody tr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"td\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mhttps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}